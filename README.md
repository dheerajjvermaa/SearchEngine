# Multi-document Embedding Search Engine

This project implements a lightweight embedding-based search engine capable of indexing text documents, caching embeddings to avoid redundant computation, and performing efficient vector similarity searches.

##  Folder Structure
The project follows a modular structure as required:

```
AI_Intern_Assignment/
├── data/
│   └── docs/            # Contains the 200 text documents (ignored by Git)
├── src/
│   ├── api.py           # FastAPI retrieval endpoint
│   ├── app_ui.py        # Streamlit User Interface (Bonus)
│   ├── cache_manager.py # SQLite caching logic
│   ├── embedder.py      # Embedding generation & text preprocessing
│   ├── search_engine.py # FAISS vector indexing & ranking logic
│   └── evaluate.py      # Script to evaluate retrieval quality
├── download_data.py     # Script to download and prepare the dataset
├── requirements.txt     # Project dependencies
└── README.md            # Project documentation
```
##  Design Choices
### Embedding Model:

Selected sentence-transformers/all-MiniLM-L6-v2 as recommended in the assignment.

Reasoning: It provides a perfect balance of speed and semantic accuracy for CPU-based environments compared to heavier BERT models.

### Vector Index:

Used FAISS (IndexFlatIP).

Reasoning: We normalize embeddings before indexing, making Inner Product (IP) equivalent to Cosine Similarity, which is the standard metric for semantic textual similarity.

### Caching Mechanism:

Implemented using SQLite.

Reasoning: SQLite is serverless, lightweight, and handles concurrent reads better than a simple JSON file or Pickle, while remaining easy to inspect.

##  How Caching Works
To satisfy the requirement of "Local caching (no recomputing embeddings)", the system uses an SQLite database (embeddings_cache.db).

### Hashing: When a document is loaded, we compute a SHA256 hash of its text content.

### Lookup: The system checks the database for an entry matching the doc_id.

### Validation:

If the doc_id exists AND the stored hash matches the current text hash, the cached embedding is returned.

If the hash differs (file changed) or doesn't exist, the embedding is regenerated by the model and the cache is updated.

##  How to Run Embedding Generation
Embedding generation is automated within the Embedder class, but you must first populate the data folder.

### 1. Prepare the Data: Run the helper script to download the "20 Newsgroups" dataset and save the required 100-200 files:

```bash

python download_data.py
```
### 2. Generate/Update Embeddings: Embeddings are generated automatically when you start the application (UI or API). The system scans data/docs, checks the cache, and processes new files in batches.

To trigger generation manually without the UI, you can run:

```bash

python -c "from src.embedder import Embedder; Embedder().generate_embeddings('data/docs')"
```
##  How to Start the API
The project includes a FastAPI implementation for the retrieval endpoint.

### 1. Run the Server:

```bash

uvicorn src.api:app --reload
```
### 2. Test the Endpoint: Send a POST request to /search:

```bash

curl -X POST "[http://127.0.0.1:8000/search](http://127.0.0.1:8000/search)" ^
     -H "Content-Type: application/json" ^
     -d "{\"query\": \"space exploration\", \"top_k\": 5}"
```
##  Streamlit UI
A user-friendly interface is available to visualize results.

### Run the App:

```bash

streamlit run src/app_ui.py
```
